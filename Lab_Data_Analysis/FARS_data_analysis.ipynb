{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48f9309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ceca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning /content/drive/MyDrive/Lab/Lab_Data_Analysis/...\n",
      "  [+] Found data for Year: 2023\n",
      "  [+] Found data for Year: 2022\n",
      "  [+] Found data for Year: 2021\n",
      "  [+] Found data for Year: 2020\n",
      "Processing 2020...\n",
      "  [?] Checking folder: /content/drive/MyDrive/Lab/Lab_Data_Analysis/2020\n",
      "      Loading: vehicle.csv + VEH_AUX.CSV + accident.csv\n",
      "Processing 2021...\n",
      "  [?] Checking folder: /content/drive/MyDrive/Lab/Lab_Data_Analysis/2021\n",
      "      Loading: vehicle.csv + VEH_AUX.CSV + accident.csv\n",
      "Processing 2022...\n",
      "  [?] Checking folder: /content/drive/MyDrive/Lab/Lab_Data_Analysis/2022\n",
      "      Loading: vehicle.csv + veh_aux.csv + accident.csv\n",
      "Processing 2023...\n",
      "  [?] Checking folder: /content/drive/MyDrive/Lab/Lab_Data_Analysis/2023\n",
      "      Loading: vehicle.csv + veh_aux.csv + accident.csv\n",
      "\n",
      "Successfully imported 235438 total records from 4 years.\n",
      "(235438, 289)\n",
      "\n",
      "Truck Crashes on Curves per Year:\n",
      "YEAR\n",
      "2020    261\n",
      "2021    334\n",
      "2022    294\n",
      "2023    293\n",
      "Name: is_truck_on_curve, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION (Drive Edition)\n",
    "# ==========================================\n",
    "CONFIG = {\n",
    "    # Update this to match your folder structure in Drive\n",
    "    # The '/content/drive/MyDrive/' part is standard for Colab\n",
    "    \"BASE_DIR\": \"/content/drive/MyDrive/Lab/Lab_Data_Analysis/\", \n",
    "\n",
    "    # We will look for these specific filenames inside the folders\n",
    "    \"RAW_FILENAME\": \"vehicle.csv\",\n",
    "    \"AUX_FILENAME\": \"VEH_AUX.csv\",\n",
    "    \"ACC_FILENAME\": \"accident.csv\",\n",
    "    \n",
    "    # Codes\n",
    "    \"CODES\": {\n",
    "        \"LARGE_TRUCK\": [4], \n",
    "        \"CURVES\": [2, 3, 4] \n",
    "    }\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. HELPER: Auto-Detect Years\n",
    "# ==========================================\n",
    "\n",
    "def find_file_insensitive(folder, target_name):\n",
    "    \"\"\"\n",
    "    Scans a folder for a file matching 'target_name', ignoring case.\n",
    "    Example: Finds 'veh_aux.csv' even if you asked for 'VEH_AUX.CSV'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # List all actual files in the directory\n",
    "        actual_files = os.listdir(folder)\n",
    "        \n",
    "        # Check them one by one\n",
    "        for f in actual_files:\n",
    "            if f.lower() == target_name.lower():\n",
    "                return os.path.join(folder, f)\n",
    "                \n",
    "        return None # Not found\n",
    "    except OSError:\n",
    "        return None\n",
    "\n",
    "def find_data_folders():\n",
    "    \"\"\"\n",
    "    Scans the BASE_DIR and finds all folders containing the required files.\n",
    "    Returns a dictionary: { 2021: 'path/to/2021', 2022: 'path/to/2022' }\n",
    "    \"\"\"\n",
    "    valid_years = {}\n",
    "    \n",
    "    # Use glob to find all 'vehicle.csv' files recursively\n",
    "    # The '**' means \"look in every subfolder\"\n",
    "    search_pattern = os.path.join(CONFIG[\"BASE_DIR\"], \"**\", CONFIG[\"RAW_FILENAME\"])\n",
    "    found_files = glob.glob(search_pattern, recursive=True)\n",
    "    \n",
    "    print(f\"Scanning {CONFIG['BASE_DIR']}...\")\n",
    "    \n",
    "    for file_path in found_files:\n",
    "        # Infer the folder path and the year from the file path\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        \n",
    "        # Try to extract a year from the folder name (e.g., \".../2021/vehicle.csv\")\n",
    "        # We take the folder name immediately containing the file\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        \n",
    "        if folder_name.isdigit() and len(folder_name) == 4:\n",
    "            year = int(folder_name)\n",
    "            valid_years[year] = folder_path\n",
    "            print(f\"  [+] Found data for Year: {year}\")\n",
    "        else:\n",
    "            print(f\"  [-] Found file in '{folder_name}' but it doesn't look like a year. Skipping.\")\n",
    "            \n",
    "    return valid_years\n",
    "\n",
    "# ==========================================\n",
    "# 3. PROCESSING PIPELINE\n",
    "# ==========================================\n",
    "def load_and_process_year(year, folder_path):\n",
    "    print(f\"  [?] Checking folder: {folder_path}\")\n",
    "    \n",
    "    # Use the smart finder for BOTH files\n",
    "    path_raw = find_file_insensitive(folder_path, CONFIG[\"RAW_FILENAME\"])\n",
    "    path_aux = find_file_insensitive(folder_path, CONFIG[\"AUX_FILENAME\"])\n",
    "    path_acc = find_file_insensitive(folder_path, CONFIG[\"ACC_FILENAME\"])\n",
    "    \n",
    "    # 2. Validation\n",
    "    if not path_raw:\n",
    "        print(f\"  [!] CRITICAL: Missing '{CONFIG['RAW_FILENAME']}'\")\n",
    "        return None\n",
    "    if not path_aux:\n",
    "        print(f\"  [!] CRITICAL: Missing '{CONFIG['AUX_FILENAME']}'\")\n",
    "        return None\n",
    "    if not path_acc:\n",
    "        print(f\"  [!] CRITICAL: Missing '{CONFIG['ACC_FILENAME']}'\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        print(f\"      Loading: {os.path.basename(path_raw)} + {os.path.basename(path_aux)} + {os.path.basename(path_acc)}\")\n",
    "        \n",
    "        # 3. Load Data (low_memory=False to prevent Dtype warnings)\n",
    "        df_raw = pd.read_csv(path_raw, encoding='latin1', low_memory=False)\n",
    "        df_aux = pd.read_csv(path_aux, encoding='latin1', low_memory=False)\n",
    "        df_acc = pd.read_csv(path_acc, encoding='latin1', low_memory=False) # <--- NEW LOAD\n",
    "        \n",
    "        # 4. Standardize Columns\n",
    "        df_raw.columns = [c.upper() for c in df_raw.columns]\n",
    "        df_aux.columns = [c.upper() for c in df_aux.columns]\n",
    "        df_acc.columns = [c.upper() for c in df_acc.columns]\n",
    "        \n",
    "        # Rename legacy columns if needed\n",
    "        if 'VALIGN' in df_raw.columns: df_raw.rename(columns={'VALIGN': 'V_ALIGN'}, inplace=True)\n",
    "        \n",
    "        # 5. MERGE STEP 1: Vehicle + Aux (Inner Join on ST_CASE + VEH_NO)\n",
    "        df_merged = pd.merge(df_raw, df_aux, on=['ST_CASE', 'VEH_NO'], how='inner', suffixes=('', '_AUX_DROP'))\n",
    "        \n",
    "        # 6. MERGE STEP 2: Result + Accident (Left Join on ST_CASE only)\n",
    "        # We use suffixes to prevent duplicate column names like STATE_x, STATE_y\n",
    "        df_final = pd.merge(df_merged, df_acc, on='ST_CASE', how='left', suffixes=('', '_ACC_DROP'))\n",
    "        \n",
    "        # Clean up: Remove duplicate columns generated by the merges\n",
    "        df_final = df_final.filter(regex='^(?!.*_DROP)')\n",
    "\n",
    "        # 7. Apply Logic / Calculations\n",
    "        df_final['YEAR'] = year\n",
    "        df_final['is_large_truck'] = df_final['A_BODY'].isin(CONFIG[\"CODES\"][\"LARGE_TRUCK\"])\n",
    "        df_final['is_curve'] = df_final['V_ALIGN'].isin(CONFIG[\"CODES\"][\"CURVES\"])\n",
    "        df_final['is_truck_on_curve'] = df_final['is_large_truck'] & df_final['is_curve']\n",
    "        \n",
    "        return df_final\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [!] Error processing {year}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION\n",
    "# ==========================================\n",
    "all_data = []\n",
    "detected_years = find_data_folders()\n",
    "\n",
    "if not detected_years:\n",
    "    print(\"No valid data folders found! Check your BASE_DIR path.\")\n",
    "else:\n",
    "    # Sort years to process in order\n",
    "    for year in sorted(detected_years.keys()):\n",
    "        print(f\"Processing {year}...\")\n",
    "        df_year = load_and_process_year(year, detected_years[year])\n",
    "        \n",
    "        if df_year is not None:\n",
    "            all_data.append(df_year)\n",
    "\n",
    "    # Combine\n",
    "    if all_data:\n",
    "        final_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\nSuccessfully imported {len(final_df)} total records from {len(all_data)} years.\")\n",
    "        print(final_df.shape)\n",
    "        \n",
    "\n",
    "        # Quick Check\n",
    "        summary = final_df.groupby('YEAR')['is_truck_on_curve'].sum()\n",
    "        print(\"\\nTruck Crashes on Curves per Year:\")\n",
    "        print(summary)\n",
    "    else:\n",
    "        print(\"Processing complete, but no data was generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
